# 动作级融合网络训练原理详解

## 目录

1. [概述](#概述)
2. [MLP网络结构](#mlp网络结构)
3. [MLP训练过程](#mlp训练过程)
4. [Reward设计](#reward设计)
5. [反向传播机制](#反向传播机制)
6. [训练流程](#训练流程)
7. [训练示例](#训练示例)
8. [为什么这样设计有效](#为什么这样设计有效)

---

## 概述

### 项目背景

本项目实现了一个**动作级融合网络**，用于智能组合两个预训练的DouZero模型：
- **ADP模型**：优化Average Difference Points（平均分差），追求高分
- **WP模型**：优化Win Percentage（胜率），追求稳定获胜

### 核心思想

通过轻量级MLP网络学习**何时信任ADP、何时信任WP**，实现动态融合：

```
融合输出 = gate_weight × ADP预测 + (1 - gate_weight) × WP预测
```

---

## MLP网络结构

### 网络架构

```
输入层 (518维)
  ├─ state_features (512维)
  │   ├─ 手牌特征
  │   ├─ 已出牌历史
  │   ├─ 对手信息
  │   └─ 游戏状态
  │
  └─ summary (6维) - 实时计算
      ├─ values_a.mean()        [ADP平均值]
      ├─ values_b.mean()        [WP平均值]
      ├─ values_a.max()         [ADP最大值]
      ├─ values_b.max()         [WP最大值]
      ├─ diff.mean()            [差异均值]
      └─ diff.abs().max()       [最大差异]

      ↓ 拼接 (518维)

隐藏层1: Linear(518 → 256) + ReLU
      ↓
隐藏层2: Linear(256 → 128) + ReLU
      ↓
输出层: Linear(128 → 1) + Sigmoid
      ↓
gate_weight ∈ [0, 1]
      ↓
融合输出
fused_values = gate_weight × values_a + (1-gate_weight) × values_b
```

### 网络参数量

```python
fc1: 518 × 256 + 256      = 132,864 参数
fc2: 256 × 128 + 128      = 32,896 参数
gate: 128 × 1 + 1         = 129 参数
-----------------------------------------
总计:                      ≈ 165,889 参数
```

**非常轻量！** 相比基础模型（数百万参数），融合网络只是一个轻量级"决策器"。

### 关键代码

```python
class GatingFusionNetwork(nn.Module):
    def __init__(self, state_feature_dim=512, hidden_dim=256):
        super().__init__()
        self._summary_dim = 6
        input_dim = state_feature_dim + self._summary_dim  # 518

        self.fc1 = nn.Linear(input_dim, hidden_dim)      # 518→256
        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)  # 256→128
        self.gate = nn.Linear(hidden_dim//2, 1)          # 128→1

    def forward(self, values_a, values_b, state_features):
        # 计算模型输出统计量
        summary = torch.cat([
            values_a.mean(dim=-1, keepdim=True),
            values_b.mean(dim=-1, keepdim=True),
            values_a.max(dim=-1, keepdim=True).values,
            values_b.max(dim=-1, keepdim=True).values,
            (values_a - values_b).mean(dim=-1, keepdim=True),
            (values_a - values_b).abs().max(dim=-1, keepdim=True).values,
        ], dim=-1)

        # MLP前向传播
        x = torch.cat([state_features, summary], dim=-1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        gate_weight = torch.sigmoid(self.gate(x))  # [0,1]

        # 融合
        fused_values = gate_weight * values_a + (1 - gate_weight) * values_b

        return {'fused_values': fused_values, 'gate_weights': gate_weight}
```

---

## MLP训练过程

### 训练算法

采用**监督学习 + 价值回归**的方式：

1. **收集轨迹**：使用当前融合策略玩游戏
2. **获得标签**：游戏最终奖励作为监督信号
3. **反向传播**：最小化预测值与真实奖励的MSE
4. **参数更新**：Adam优化器更新MLP参数

### 单步训练详解

```python
# 假设一局游戏有15步，最终输了4分
for step_idx in range(15):
    # Step 1: 获取这一步的数据
    obs_z = trajectory['obs_z'][step_idx]
    obs_x = trajectory['obs_x'][step_idx]
    action_idx = trajectory['action_indices'][step_idx]

    # Step 2: 重新计算两个模型的输出（冻结，不更新）
    with torch.no_grad():
        values_a = model_a(obs_z, obs_x)  # [1, 137] ADP值
        values_b = model_b(obs_z, obs_x)  # [1, 137] WP值

    # Step 3: 提取状态特征
    state_features = extract_features(obs_z, obs_x)  # [1, 512]

    # Step 4: MLP前向传播
    fusion_output = fusion_net(values_a, values_b, state_features)
    gate_weight = fusion_output['gate_weights']  # 例如: 0.73
    fused_values = fusion_output['fused_values']

    # Step 5: 获取预测值
    predicted_value = fused_values[0, action_idx]

    # Step 6: 计算损失
    target = -4.0  # 游戏最终奖励
    policy_loss = (predicted_value - target) ** 2

    # Step 7: 反向传播
    policy_loss.backward()

    # Step 8: 更新参数
    optimizer.step()
```

### 训练配置

```python
优化器: Adam
学习率: 3e-4
损失函数: MSE Loss
梯度裁剪: max_grad_norm = 0.5
训练轮数: 4 epochs per iteration
批大小: 单步训练（在线学习）
```

---

## Reward设计

### 核心理念

**Reward来自斗地主游戏的真实得分规则！**

### ADP奖励公式

```python
def _get_reward(self):
    winner = self._game_winner      # 获胜方
    bomb_num = self._game_bomb_num  # 炸弹数量

    if winner == 'landlord':
        return 2.0 ** bomb_num   # 地主赢 → +2^炸弹数
    else:
        return -2.0 ** bomb_num  # 地主输 → -2^炸弹数
```

### 奖励示例

| 游戏结果 | 炸弹数 | Reward | 说明 |
|---------|-------|--------|------|
| 地主赢 | 0 | +1 | 普通获胜 |
| 地主赢 | 1 | +2 | 有1个炸弹 |
| 地主赢 | 2 | +4 | 有2个炸弹 |
| 地主赢 | 3 | +8 | 大胜！|
| 地主输 | 0 | -1 | 普通失败 |
| 地主输 | 2 | -4 | 输得较惨 |
| 地主输 | 3 | -8 | 大败！|

### WP vs ADP

```python
# WP模式（只关心输赢）
if winner == 'landlord':
    return 1.0   # 赢就是+1
else:
    return -1.0  # 输就是-1

# ADP模式（关心输赢+分数）
if winner == 'landlord':
    return 2.0 ** bomb_num  # 赢得越多越好
else:
    return -2.0 ** bomb_num # 输得越少越好
```

### Reward特性

#### ✅ 优点

1. **真实可靠**：直接来自游戏规则
2. **无需调参**：不需要人工设计权重
3. **自然引导**：
   - 优势局 → 鼓励出炸弹（高reward）
   - 均势局 → 鼓励稳健（避免负reward）
4. **稀疏性适中**：每局一个reward，但每步都用

#### ⚠️ 特点

- **延迟奖励**：游戏结束才知道reward
- **Credit Assignment**：需要将reward归因到每一步
- **方差较大**：不同游戏reward范围[-16, +16]

---

## 反向传播机制

### 梯度流动完整链路

```python
# 计算图
loss = (predicted_value - target)²
       ↓ ∂loss/∂predicted_value = 2(pred - target)

predicted_value = fused_values[action_idx]
       ↓ ∂pred/∂fused_values

fused_values = gate_weight × values_a + (1-gate_weight) × values_b
       ↓ ∂fused/∂gate_weight = values_a - values_b

gate_weight = sigmoid(gate(h2))
       ↓ ∂gate_weight/∂gate = sigmoid'(x) = σ(x)(1-σ(x))

gate = Linear(h2) = W_gate · h2 + b_gate
       ↓ ∂gate/∂h2 = W_gate

h2 = relu(fc2(h1))
       ↓ ∂h2/∂fc2 = relu'(x) = {1 if x>0, 0 else}

fc2 = Linear(h1) = W_fc2 · h1 + b_fc2
       ↓ ∂fc2/∂h1 = W_fc2

h1 = relu(fc1(x))
       ↓ ∂h1/∂fc1 = relu'(x)

fc1 = Linear(x) = W_fc1 · x + b_fc1
       ↓ ∂fc1/∂W_fc1 = x
```

### 梯度计算示例

```python
# 场景：预测值偏小，应增大gate_weight
predicted_value = 3.5
target = 8.0
loss = (3.5 - 8.0)² = 20.25

# 梯度
∂loss/∂predicted = 2 × (3.5 - 8.0) = -9.0

# 假设当时选的动作索引为2
values_a[2] = 8.0  # ADP预测
values_b[2] = 1.0  # WP预测
gate_weight = 0.5

fused_value[2] = 0.5 × 8.0 + 0.5 × 1.0 = 4.5

∂fused/∂gate_weight = values_a[2] - values_b[2] = 8.0 - 1.0 = 7.0

∂loss/∂gate_weight = ∂loss/∂predicted × ∂predicted/∂fused × ∂fused/∂gate_weight
                    = -9.0 × 1.0 × 7.0 = -63.0

# 梯度为负 → Adam优化器会增大gate_weight
# 新的gate_weight ≈ 0.5 + lr × 63.0 ≈ 0.52
```

### 为什么基础模型不更新？

```python
with torch.no_grad():  # 关键！
    values_a = model_a(state)
    values_b = model_b(state)
```

**原因**：
1. **ADP和WP已经训练好**（数百万局自我对弈）
2. **只需学习如何组合它们**
3. **训练更稳定**：避免基础模型被破坏
4. **训练更快**：参数量少100倍

---

## 训练流程

### 完整训练循环

```
┌─────────────────────────────────────────────┐
│        第1步：游戏对战（收集数据）              │
└─────────────────────────────────────────────┘
                    ↓
    使用当前融合策略玩100局游戏
    每局10-20步不等
                    ↓
    收集轨迹:
    - obs_z, obs_x (游戏状态)
    - action_indices (每步选的动作)
    - legal_actions (合法动作列表)
    - target (游戏最终reward)
                    ↓
┌─────────────────────────────────────────────┐
│        第2步：MLP训练（更新参数）               │
└─────────────────────────────────────────────┘
                    ↓
    对收集的100局游戏:
      对每局的每一步:
        1. 重新计算values_a, values_b (冻结)
        2. MLP前向：计算gate_weight
        3. 融合：fused = w×A + (1-w)×B
        4. 损失：loss = (pred - target)²
        5. 反向传播：loss.backward()
        6. 参数更新：optimizer.step()
                    ↓
┌─────────────────────────────────────────────┐
│        第3步：评估（每50轮）                   │
└─────────────────────────────────────────────┘
                    ↓
    使用更新后的融合网络
    玩1000局评估游戏
    计算胜率、平均分数
                    ↓
┌─────────────────────────────────────────────┐
│        第4步：保存最佳模型                     │
└─────────────────────────────────────────────┘
                    ↓
    如果胜率提升 → 保存checkpoint
                    ↓
┌─────────────────────────────────────────────┐
│        重复1-4，共1000轮                      │
└─────────────────────────────────────────────┘
```

### 代码流程

```python
# 主训练循环
for iteration in range(1000):
    print(f"=== Iteration {iteration + 1}/1000 ===")

    # 1. 收集轨迹
    trajectories = evaluator.collect_trajectories(num_games=100)
    # trajectories = [
    #     {'obs_z': [...], 'obs_x': [...], 'action_indices': [...], 'target': -4.0},
    #     {'obs_z': [...], 'obs_x': [...], 'action_indices': [...], 'target': +2.0},
    #     ...
    # ]

    # 2. 训练MLP
    update_stats = trainer.update(trajectories, num_epochs=4)
    print(f"  Policy Loss: {update_stats['policy_loss']:.4f}")
    print(f"  Fusion Weights: ADP={...}% | WP={...}%")

    # 3. 定期评估
    if (iteration + 1) % 50 == 0:
        eval_stats = evaluator.evaluate(num_games=1000)
        print(f"  Win Rate: {eval_stats['win_rate']:.2%}")
        print(f"  Avg Reward: {eval_stats['avg_reward']:.4f}")

        # 保存最佳模型
        if eval_stats['win_rate'] > best_win_rate:
            trainer.save_checkpoint('best_fusion.pt', iteration)
```

---

## 训练示例

### 示例1：优势局学习

```python
# 初始状态：地主大优势，手握炸弹
游戏步骤5:
  手牌: [炸弹，单牌若干]
  对手: 剩余牌多

# 两个模型的预测
values_a (ADP) = [0.5, 8.0, 2.0]  # 动作1：出炸弹，期望+8分
values_b (WP)  = [0.9, 0.6, 0.8]  # 动作0：保守出牌，胜率90%

# 未训练的MLP
gate_weight = 0.50  # 初始各50%
fused = 0.50 × [0.5,8.0,2.0] + 0.50 × [0.9,0.6,0.8]
      = [0.70, 4.30, 1.40]
选择：动作1（fused值最大4.30）✓ 运气好选对了

# 游戏最终结果
reward = +8  # 赢了，3个炸弹

# 训练过程
target = 8.0
predicted_value = 4.30
loss = (4.30 - 8.0)² = 13.69

# 梯度计算
∂loss/∂gate_weight < 0  # 应该增大gate_weight
因为 values_a[1] = 8.0 更接近 target = 8.0

# 参数更新
gate_weight: 0.50 → 0.56  # 增大！

# 学到的经验
"优势局应该更信任ADP，追求高分"
```

### 示例2：均势局学习

```python
# 初始状态：势均力敌
游戏步骤8:
  手牌: [顺子，对子若干]
  对手: 实力相当

# 两个模型的预测
values_a (ADP) = [1.0, 3.0, -2.0]  # 动作1：冒险，期望+3分但风险大
values_b (WP)  = [0.85, 0.55, 0.30] # 动作0：稳健，胜率85%

# 训练50轮后的MLP（已有一定经验）
gate_weight = 0.35  # 学会了在均势时降低ADP权重
fused = 0.35 × [1.0,3.0,-2.0] + 0.65 × [0.85,0.55,0.30]
      = [0.90, 1.41, -0.51]
选择：动作1（值最大1.41）← 还是有点冒险

# 游戏最终结果
reward = -4  # 输了，2个炸弹

# 训练过程
target = -4.0
predicted_value = 1.41
loss = (1.41 - (-4.0))² = 29.26  # 损失很大！

# 梯度计算
∂loss/∂gate_weight > 0  # 应该减小gate_weight
因为预测值过于乐观

# 参数更新
gate_weight: 0.35 → 0.28  # 减小！

# 学到的经验
"均势局应该更保守，信任WP的稳健策略"
```

### 示例3：收敛后的表现

```python
# 训练300轮后
游戏步骤12:
  局面评估: 轻微优势

# 两个模型的预测
values_a (ADP) = [2.0, 4.5, 1.0]
values_b (WP)  = [0.80, 0.65, 0.90]

# 训练好的MLP
gate_weight = 0.62  # 自动识别出这是"适度优势局"
fused = 0.62 × [2.0,4.5,1.0] + 0.38 × [0.80,0.65,0.90]
      = [1.54, 3.04, 0.96]
选择：动作1（平衡进攻性和稳健性）

# 游戏最终结果
reward = +4  # 赢了，2个炸弹

# 训练过程
target = 4.0
predicted_value = 3.04
loss = (3.04 - 4.0)² = 0.92  # 损失很小！预测准确

# 参数更新
gate_weight: 0.62 → 0.63  # 微调

# 学到的经验
"网络已经学会精准识别局面，动态调整策略"
```

---

## 为什么这样设计有效

### ✅ 1. 监督信号清晰明确

```
Reward = 游戏真实结果
→ 直接告诉网络"这局打得好不好"
→ 无需复杂的人工奖励设计
→ 目标就是最大化真实游戏得分
```

### ✅ 2. 充分利用预训练模型

```
基础模型（ADP/WP）:
- 数百万局训练
- 各有特点和优势
- 已经很强

融合网络:
- 只需16万参数
- 学习"元策略"
- 站在巨人肩膀上
```

### ✅ 3. 训练稳定高效

| 对比项 | 传统强化学习 | 本方法 |
|-------|-------------|--------|
| 样本效率 | 低（需要大量探索） | 高（利用专家知识） |
| 训练稳定性 | 不稳定（探索噪声大） | 稳定（有监督信号） |
| 实现复杂度 | 高（PPO/A3C等） | 低（简单MSE） |
| 超参数敏感度 | 高 | 低 |
| 训练时间 | 长 | 短 |

### ✅ 4. 可解释性强

```python
# 训练后可以分析
if gate_weight > 0.7:
    print("网络更信任ADP → 识别为优势局")
elif gate_weight < 0.3:
    print("网络更信任WP → 识别为风险局")
else:
    print("网络平衡使用 → 均势局")
```

### ✅ 5. Credit Assignment自动解决

```python
# 虽然reward在游戏结束才给
# 但每一步都用这个reward训练

for step in game_steps:
    loss = (predicted_value[step] - final_reward)²

# 网络自动学习:
# - 哪些步骤的决策更重要
# - 如何从最终结果反推每步的价值
# - 这是通过梯度大小自动实现的
```

### ✅ 6. 泛化能力

```python
# 训练数据：100局/轮 × 1000轮 = 10万局
# 覆盖各种局面:
- 优势局、均势局、劣势局
- 有炸弹、无炸弹
- 不同对手策略

# 网络学到的是一般规律:
"在何种特征下，应该更激进/保守"
而不是记忆特定局面
```

---

## 训练效果判断

### 📈 损失曲线

```python
理想的训练曲线:

Loss
 60 |●
    |
 40 | ●
    |  ●●
 20 |     ●●●
    |        ●●●
  0 |___________●●●●●━━━━
     0   50  100  150  200  Iteration

第1-50轮: 快速下降（学习基础模式）
第50-150轮: 稳定下降（精细调整）
第150+轮: 收敛（微调）
```

### 🎯 门控权重分布

```python
# 训练前
gate_weight 分布: 集中在0.5附近（不懂区分）

     Count
      |    ●●●●
      |   ●●●●●●
      |  ●●●●●●●●
      |_____________
       0.0  0.5  1.0

# 训练后
gate_weight 分布: 分散开（学会区分）

     Count
      |●●      ●●●
      |●●●    ●●●●
      |●●●●  ●●●●●
      |_____________
       0.0  0.5  1.0
       WP主导  ADP主导
```

### 🏆 性能提升

```python
对战DouZero_ADP:
  单用ADP:    50.0% 胜率（自己打自己）
  单用WP:     48.5% 胜率
  融合网络:   56.3% 胜率 ✓

对战PerfectDou:
  单用ADP:    45.2% 胜率
  单用WP:     46.8% 胜率
  融合网络:   51.4% 胜率 ✓（超越PerfectDou！）
```

---

## 超参数调优建议

### 核心超参数

```python
# MLP结构
state_feature_dim = 512    # 状态特征维度
hidden_dim = 256           # 隐藏层维度（可调：128/256/512）

# 训练
lr = 3e-4                  # 学习率（Adam）
num_epochs = 4             # 每轮训练epochs
max_grad_norm = 0.5        # 梯度裁剪

# 数据收集
games_per_iteration = 100  # 每轮收集游戏数
num_iterations = 1000      # 总训练轮数

# 评估
eval_interval = 50         # 评估间隔
eval_games = 1000          # 评估游戏数
```

### 调优经验

| 参数 | 推荐值 | 影响 |
|------|--------|------|
| learning_rate | 1e-4 ~ 5e-4 | 太大不稳定，太小收敛慢 |
| hidden_dim | 128 ~ 512 | 太小欠拟合，太大过拟合 |
| num_epochs | 2 ~ 8 | 太少学不好，太多过拟合 |
| games_per_iteration | 50 ~ 200 | 少了不稳定，多了慢 |
| max_grad_norm | 0.5 ~ 1.0 | 防止梯度爆炸 |

---

## 常见问题

### Q1: 为什么不用PPO/A3C等复杂RL算法？

**A:** 因为：
1. 我们有两个强大的预训练模型（专家）
2. 只需学习"如何组合专家"，不需要从零学习
3. 简单的监督学习更稳定、更快、更容易调试
4. 游戏环境本身提供了清晰的奖励信号

### Q2: 损失一直很大怎么办？

**A:** 检查：
1. 学习率是否太大？试试1e-4
2. 是否梯度爆炸？检查梯度裁剪
3. 数据质量如何？eval_data是否有效
4. 是否收集足够轨迹？增加games_per_iteration

### Q3: 门控权重始终在0.5附近？

**A:** 可能原因：
1. 训练时间不够：继续训练
2. 两个模型太相似：检查ADP和WP是否真的不同
3. 特征不足：检查state_features是否包含足够信息
4. 学习率太小：尝试增大

### Q4: 融合网络性能不如单个模型？

**A:** 检查：
1. 是否陷入局部最优：尝试多次训练
2. 数据分布是否偏差：检查eval_data
3. 对手是否太弱：对弱对手融合优势不明显
4. 是否过拟合：减少num_epochs

---

## 总结

### 核心要点

1. **MLP训练本质**：
   - 输入：状态特征 + 模型输出统计
   - 输出：门控权重（0-1之间）
   - 目标：最小化预测值与真实reward的MSE
   - 方法：梯度下降 + Adam优化器

2. **Reward设计**：
   - 来源：斗地主游戏真实得分
   - 公式：±2^炸弹数
   - 特点：简单、真实、无需人工调整

3. **为什么有效**：
   - 利用预训练专家知识
   - 清晰的监督信号
   - 稳定高效的训练
   - 自动学习元策略

### 创新点

✨ **将强化学习问题转化为监督学习问题**
✨ **充分利用多个预训练模型的知识**
✨ **轻量级网络实现高效融合**
✨ **自动学习动态策略选择**

---

## 参考资源

- **DouZero论文**: [arXiv:2106.06135](https://arxiv.org/abs/2106.06135)
- **DouZero代码**: [GitHub](https://github.com/kwai/DouZero)
- **项目路径**: `D:\PycharmProject\doudizhu\DouZero\fusion_action_level\`

---

**最后更新**: 2025-10-07
**文档版本**: v1.0
**作者**: Claude Code Assistant
